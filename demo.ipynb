{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo of SE-Gym\n",
    "This is a demo of running LLM-Prompt-based agents in the SE-Gym environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import se_gym\n",
    "import importlib\n",
    "import dotenv\n",
    "import logging\n",
    "\n",
    "importlib.reload(se_gym.api)\n",
    "\n",
    "dotenv.load_dotenv(\"./se_gym/.env\")\n",
    "\n",
    "env = se_gym.api.make(\"dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TIME_STEPS = 20\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s %(levelname)s:%(message)s\",\n",
    "    level=logging.INFO,\n",
    "    datefmt=\"%I:%M:%S\",\n",
    "    handlers=[logging.FileHandler(\"se_gym.log\"), logging.StreamHandler()],\n",
    ")\n",
    "logging.getLogger(\"caller\").setLevel(level=logging.DEBUG)\n",
    "logging.getLogger(\"dockerconnector\").setLevel(level=logging.DEBUG)\n",
    "logging.getLogger(\"genetic\").setLevel(level=logging.DEBUG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import se_gym.genetic\n",
    "\n",
    "state = env.reset()\n",
    "\n",
    "# Multiple initial prompts, as we are using a genetic algorithm\n",
    "INITIAL_θ = [\n",
    "    \"You are a Software engineer. Suggest Code to fix the issue. Use the provided code snippet to understand the issue. Write tests to verify your fix.\",\n",
    "    \"Fix the issue.\",\n",
    "    \"The code is broken, as described in the provided code snippet. Fix it. Write tests to verify your fix.\",\n",
    "]\n",
    "\n",
    "\n",
    "se_gym.config.MODEL_NAME = \"llama3:8b\"\n",
    "\n",
    "# Add your client here\n",
    "client = se_gym.openai_lmu.get_lmu_openai_client()\n",
    "\n",
    "π = se_gym.Sampler(client, code_base_root=env.reset().path)\n",
    "\n",
    "population = se_gym.genetic.Population(\n",
    "    client=client,\n",
    "    initial_individuals=INITIAL_θ,\n",
    "    elite_size=0,  # No elitism\n",
    "    mutation_rate=0.2,\n",
    "    crossover_rate=0.7,\n",
    "    sampler=π,\n",
    ")\n",
    "\n",
    "observer = se_gym.ManualObserver(\n",
    "    files=[\n",
    "        \"./temp/kyrillschmidpythonenv/src/python_env/__main__.py\",\n",
    "        \"./temp/kyrillschmidpythonenv/tests/my_test.py\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "R = se_gym.fitness.num_failed_tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(1):\n",
    "    r = 0\n",
    "    s_t = env.reset()\n",
    "    for t in range(MAX_TIME_STEPS):\n",
    "        o_t = observer(s_t)  # observation at time t\n",
    "        a_t = population.sample(o_t)  # actions at time t\n",
    "        s_t = env.step(a_t)  # apply actions at time t to get next state\n",
    "        current_r = [R(s_) for s_ in s_t]\n",
    "        r += sum(current_r)\n",
    "        print(f\"Current reward: {current_r}\")\n",
    "        # evolve the population based on the current reward\n",
    "        population.evolve(current_r) \n",
    "    ## evolve the population based on the total reward \n",
    "    # population.evolve(r) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "se_gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
